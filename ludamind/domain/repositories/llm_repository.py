"""
LLM Repository Interface

This module defines the abstract interface for Large Language Model operations.
All LLM implementations (OpenAI, Anthropic, local models, etc.) must implement this interface.

Following SOLID principles:
- Single Responsibility: Only defines LLM operation contracts
- Open/Closed: Can add new LLM providers without modifying existing code
- Dependency Inversion: Domain depends on this abstraction, not concrete LLM clients
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, AsyncGenerator, Dict, List, Optional, Union
from datetime import datetime
from enum import Enum


class DatabaseType(str, Enum):
    """Enumeration for database types."""
    MYSQL = "mysql"
    MONGODB = "mongodb"


@dataclass
class QuerySpec:
    """
    Specification for a database query generated by an LLM.

    This value object contains all information needed to execute a query.
    """
    query: str  # The actual query (SQL or MongoDB query)
    params: Dict[str, Any]  # Parameters for the query
    explanation: str  # Human-readable explanation of what the query does
    database_type: DatabaseType  # Which database to execute against
    confidence: float  # Confidence score (0-1) of the generated query
    estimated_cost: Optional[float] = None  # Estimated query cost if available


@dataclass
class Message:
    """Represents a message in a conversation with an LLM."""
    role: str  # 'system', 'user', or 'assistant'
    content: str  # The message content
    timestamp: Optional[datetime] = None


@dataclass
class LLMResponse:
    """Response from an LLM operation."""
    content: str  # The response content
    tokens_used: int  # Number of tokens consumed
    cost: float  # Cost in USD
    model: str  # Model used for generation
    latency_ms: float  # Response time in milliseconds


class LLMRepository(ABC):
    """
    Abstract repository for LLM operations.

    This interface defines the contract that all LLM implementations must follow.
    It ensures that the domain layer can work with any LLM provider without knowing
    the specific implementation details.
    """

    @abstractmethod
    async def generate_query(
        self,
        question: str,
        database_type: DatabaseType,
        context: Optional[str] = None,
        examples: Optional[List[Dict[str, str]]] = None,
        temperature: float = 0.1
    ) -> QuerySpec:
        """
        Generate a database query from a natural language question.

        Args:
            question: The natural language question
            database_type: Type of database to generate query for
            context: Optional context about the database schema or business rules
            examples: Optional few-shot examples
            temperature: Temperature for generation (0.0 = deterministic, 1.0 = creative)

        Returns:
            QuerySpec containing the generated query and metadata

        Raises:
            LLMGenerationError: If query generation fails
            LLMRateLimitError: If rate limit is exceeded
            LLMAuthenticationError: If authentication fails
        """
        pass

    @abstractmethod
    async def generate_answer(
        self,
        question: str,
        results: List[Dict[str, Any]],
        context: Optional[str] = None,
        language: str = "es",
        max_length: int = 500
    ) -> str:
        """
        Generate a natural language answer from query results.

        Args:
            question: The original question
            results: Query results to transform into natural language
            context: Optional context for answer generation
            language: Language for the answer ('es' for Spanish, 'en' for English)
            max_length: Maximum length of the answer

        Returns:
            Natural language answer string

        Raises:
            LLMGenerationError: If answer generation fails
        """
        pass

    @abstractmethod
    async def chat(
        self,
        messages: List[Message],
        stream: bool = False,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
    ) -> Union[LLMResponse, AsyncGenerator[str, None]]:
        """
        Have a conversation with the LLM.

        Args:
            messages: Conversation history
            stream: Whether to stream the response
            temperature: Temperature for generation
            max_tokens: Maximum tokens to generate

        Returns:
            If stream=False: Complete LLMResponse
            If stream=True: Async generator yielding response chunks

        Raises:
            LLMGenerationError: If chat generation fails
        """
        pass

    @abstractmethod
    async def validate_query(
        self,
        query: str,
        database_type: DatabaseType
    ) -> Dict[str, Any]:
        """
        Validate if a query is safe and correct.

        Args:
            query: The query to validate
            database_type: Type of database

        Returns:
            Dictionary containing:
            - is_valid: Boolean indicating if query is valid
            - is_safe: Boolean indicating if query is safe (no destructive operations)
            - errors: List of validation errors
            - warnings: List of warnings

        Raises:
            LLMGenerationError: If validation fails
        """
        pass

    @abstractmethod
    async def explain_query(
        self,
        query: str,
        database_type: DatabaseType,
        language: str = "es"
    ) -> str:
        """
        Explain what a database query does in natural language.

        Args:
            query: The query to explain
            database_type: Type of database
            language: Language for explanation

        Returns:
            Natural language explanation of the query

        Raises:
            LLMGenerationError: If explanation generation fails
        """
        pass

    @abstractmethod
    async def optimize_query(
        self,
        query: str,
        database_type: DatabaseType,
        performance_metrics: Optional[Dict[str, Any]] = None
    ) -> QuerySpec:
        """
        Optimize a database query for better performance.

        Args:
            query: The query to optimize
            database_type: Type of database
            performance_metrics: Optional current performance metrics

        Returns:
            Optimized QuerySpec

        Raises:
            LLMGenerationError: If optimization fails
        """
        pass

    @abstractmethod
    async def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the LLM model being used.

        Returns:
            Dictionary containing:
            - model: Model name/identifier
            - provider: Provider name (OpenAI, Anthropic, etc.)
            - version: Model version
            - capabilities: List of capabilities
            - cost_per_token: Cost information
            - rate_limits: Rate limit information
        """
        pass

    @abstractmethod
    async def estimate_cost(
        self,
        text: str,
        operation_type: str = "generation"
    ) -> float:
        """
        Estimate the cost of an operation before executing it.

        Args:
            text: The text to process
            operation_type: Type of operation ('generation', 'chat', 'embedding')

        Returns:
            Estimated cost in USD

        Raises:
            LLMCostEstimationError: If cost cannot be estimated
        """
        pass

    @property
    @abstractmethod
    def is_available(self) -> bool:
        """
        Check if the LLM service is currently available.

        Returns:
            True if service is available, False otherwise
        """
        pass

    @property
    @abstractmethod
    def supports_streaming(self) -> bool:
        """
        Check if the LLM supports streaming responses.

        Returns:
            True if streaming is supported, False otherwise
        """
        pass

    @property
    @abstractmethod
    def max_context_length(self) -> int:
        """
        Get the maximum context length supported by the model.

        Returns:
            Maximum number of tokens in context
        """
        pass